{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Method in Machine Learning:\n",
    " What is Ensemble Learning?\n",
    "<img src='1e.png'>\n",
    "Ensemble Learning is a process in which multiple machine learning models(such as classifiers) are used to solved particular problem,you can see above picture where data is trained and tested using different machine learning models where final prediction is decided on the basis of `different model results`. \n",
    "\n",
    "#### Why Ensemble Learning?\n",
    "\n",
    "Ensemble methods helps improve machine learning results by combining multiple models. Using ensemble methods allows to produce better predictions compared to a single model. Therefore, the ensemble methods placed first in many prestigious machine learning competitions, such as Netflix Competition, KDD 2009, and Kaggle.\n",
    "\n",
    "The error emerging from any machine model can be broken down in to tree components mathematically. \n",
    "##### `Bias+Variance+Irreducible error`\n",
    "<img src='compare.png'>\n",
    "\n",
    "<img src='biasvariance.png'>\n",
    "In the above image you can observe that bias and variance are Inverse proportion of eatch other when bias is high then variance is low or we can say that model complexity is also less, same way if bias is low and variance is high then model complexity is high so we choose a sweet point of bias and variance where model is perfect for better accuracy. \n",
    "\n",
    "#### Types of Ensemble Learning:\n",
    "- Basic Ensemble Techniques\n",
    "  - Max Voting\n",
    "  - Averaging\n",
    "  - Weighted Average\n",
    "\n",
    "- Advance Ensemble Techniques\n",
    "  - Stacking\n",
    "  - Blending\n",
    "  - Bagging\n",
    "  - Boosting\n",
    "\n",
    "- Algorithm Based On Bagging\n",
    "  - Bagging Meta-Estimator\n",
    "  - Random Forest\n",
    "- Boosting Algorithm\n",
    "  - AdaBoost\n",
    "  - Gradient Boosting\n",
    "  - XGBoost\n",
    "  - Light GBM\n",
    "  - Catboost\n",
    "  \n",
    "#### Max Voting\n",
    "The max voting method is generally used for classification problems.In this technique,multiple models are used to make predictions for eatch data point means we vote for maximum number of class and then decide final prediction.\n",
    "#### Averaging\n",
    "Here we took average of result achieved by different model and then predict final regressor value.\n",
    "#### Weighted Average\n",
    "This is an extention of averaging method. All models are assigned different weights defining the importance of each model for prediction. \n",
    "#### Bagging:\n",
    "<img src='bag.png'>\n",
    "Bagging is sampling technique in which we create subsets of observation from the original dataset,with replacement the size of the subsets is same as the size of original set. In the above picture original data is converted in to many subsets without change it's original size then these subsets data is trained using different different classifier(Machine Learning Model) and then after that final prediction is decided using maximum voting and averaging method.\n",
    "\n",
    "##### Boosting:\n",
    "<img src='boos.JPG'>\n",
    "Boosting is a sequential process,where each subsequent model attempts to correct the errors of privious model.The succeeding models are dependent on the privious model.In this picture you can see that first we trained original data set D1,the error achieved by first model is feed in to second model after identified second model error we trained another model and then we finally mix all these model together and achieved final classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm implementation in sklearn\n",
    "Here I implement these machine learning algorithm and then compare their accuracy.\n",
    "- Bagging\n",
    "  - Random Forest\n",
    "- Boosting\n",
    "  - AdaBoost\n",
    "  - Gradient Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()\n",
    "X =cancer.data\n",
    "y = cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(n_estimators=200,random_state=1)\n",
    "abc=AdaBoostClassifier(n_estimators=200,learning_rate=0.01,random_state=1)\n",
    "gbc=GradientBoostingClassifier(n_estimators=200,learning_rate=0.01,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=1, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train,y_train)\n",
    "abc.fit(X_train,y_train)\n",
    "gbc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predrfc = rfc.predict(X_test)\n",
    "y_predabc = abc.predict(X_test)\n",
    "y_predgbc = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9440559440559441\n",
      "0.9370629370629371\n",
      "0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_predrfc,y_test))\n",
    "print(metrics.accuracy_score(y_predabc,y_test))\n",
    "print(metrics.accuracy_score(y_predgbc,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see for this problem gradiantboost classifier has more accuracy score than adaboost and randomforest classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
